server.port: 7003
server.error.include-message: always

# It is used to give each microservice a virtual hostname, a name used by the Eureka service to identify each
# microservice. Eureka clients will use this virtual hostname in the URLs that are used to make HTTP
# calls to the microservice.
# The application name property has been moved to the application.yml inside the
# microservices/review.yml
#spring.application.name: review


# With the database in place, we now need to set up the configuration for the core microservices so they
# know how to connect to their databases. This is set up in each core microservice’s configuration file,
# application.yml
# By default, Hibernate will be used by Spring Data JPA as JPA’s EntityManager.
# Strongly recommend to set this property to "none" in a production environment!
# The spring.jpa.hibernate.ddl-auto property is used to tell Spring Data JPA to create new or update existing SQL tables during startup.
spring.jpa.hibernate.ddl-auto: update

spring.datasource:
  url: jdbc:mysql://localhost/review-db
  username: user
  password: '{cipher}24b93c790fa2ed0dc5a6a2b8c39853cf4d6cd6100c248de5f279fb5dd78e7be6'

# This means that the Spring Boot application will wait for up to 60 seconds during startup to establish a database connection.
spring.datasource.hikari.initializationFailTimeout: 60000

# this specifies which spring function bean we want to run, message processor bean that I have defined. That means
# whatever message come from rabbitMq or Kafka, spring cloud stream will invoke this function.
spring.cloud.function.definition: messageProcessor

# Means my message processor has to consume message from the recommendation topic, for this I have defined
# a binding. Mere messageProcessor jo ke -in- input type ka hai usko destination:recommendations queue/topic ke
# sath bind kardo. Ye hum springCloud stream ko bata rahy hain.
spring.cloud.stream:
  defaultBinder: rabbit #binder means messaging system, by default we will use rabbitMQ
  default.contentType: application/json
  bindings.messageProcessor-in-0: # Binding = connection between your app function and the message destination (topic/queue).
    destination: reviews  # Name of the topic/queue where message will be consumed.
    group: reviewsGroup

spring.cloud.stream.bindings.messageProcessor-in-0.consumer:
  maxAttempts: 3
  backOffInitialInterval: 500
  backOffMaxInterval: 1000
  backOffMultiplier: 2.0

spring.cloud.stream.rabbit.bindings.messageProcessor-in-0.consumer:
  autoBindDlq: true
  republishToDlq: true

spring.cloud.stream.kafka.bindings.messageProcessor-in-0.consumer:
  enableDlq: true

spring.cloud.stream.kafka.binder:
  brokers: 127.0.0.1
  defaultBrokerPort: 9092

spring.rabbitmq:
  host: 127.0.0.1
  port: 5672
  username: guest
  password: '{cipher}17fcf0ae5b8c5cf87de6875b699be4a1746dd493a99d926c7a26a68c422117ef'

logging:
  level:
    root: INFO
    com.microservices: INFO
    org.hibernate.SQL: INFO
    org.hibernate.type.descriptor.sql.BasicBinder: TRACE

management.endpoint.health.show-details: "ALWAYS"
management.endpoints.web.exposure.include: "*"
# A liveness probe tells Kubernetes if a Pod needs to be replaced, and a readiness probe tells Kubernetes
# if its Pod is ready to accept requests. To simplify this work, Spring Boot has added support to implement
# liveness and readiness probes. The probes are exposed on the URLs /actuator/health/liveness and
# /actuator/health/readiness respectively. They can either be declared by configuration or implementation
# in source code, if increased control is required compared to what configuration gives.
# When declaring the probes by configuration, a health group can be declared for each probe,
# specifying what existing health indicators it should include. For example, a readiness probe should
# report DOWN if a microservice can’t access its MongoDB database. In this case, the health group for the
# readiness probe should include the mongo health indicator.
management.endpoint.health.probes.enabled: true
management.endpoint.health.group.readiness.include: readinessState, rabbit, db, mongo

# Configuration for using Micrometer Tracing and Zipkin is added to the configuration file, config-repo/*.yml.
# In the default profile, it is specified that trace information will be sent to Zipkin using the following URL.
management.zipkin.tracing.endpoint: http://zipkin-server:9411/api/v2/spans

# By default, Micrometer Tracing only sends 10% of the traces to Zipkin. To ensure that all traces are
# sent to Zipkin, the following property is added to the default profile
management.tracing.sampling.probability: 1.0

# We also want trace and span IDs to be written to logs; this will enable us to correlate log output from
# cooperating microservices that, for example, fulfill a request sent to the external API.
# We can include the trace and span IDs in the log output by specifying the following log format:
# With the above log format, the log output will look like:
# 2023-04-22T14:02:07.417Z  INFO [product-composite,01234,56789]
# Where product-composite is the name of the microservice, 01234 is the trace ID, and 56789 is the span ID.
logging.pattern.level: "%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]"

server.shutdown: graceful
spring.lifecycle.timeout-per-shutdown-phase: 10s

# yaha per baki sari configuration default profile wali ajae gi except jo neechay define ho rhaa ha wo bas override
# hoga default profile ke upar.
# Values in a profile override values from the default profile. By using YAML files, multiple Spring profiles
#  can be placed in the same file, separated by ---
---

# To handle the different configurations that are required when running locally without Docker and
# when running the microservices as Docker containers, we will use Spring profiles. Now, we will create a new
# Spring profile named docker to be used when we run our microservices as containers in Docker.


spring.config.activate.on-profile: docker

server.port: 8080

spring.datasource:
  url: jdbc:mysql://mysql/review-db

spring.rabbitmq.host: rabbitmq
spring.cloud.stream.kafka.binder.brokers: kafka


---
spring.config.activate.on-profile: streaming_partitioned

spring.cloud.stream.bindings.messageProcessor-in-0.consumer:
  partitioned: true
  instanceCount: 2

---
spring.config.activate.on-profile: streaming_instance_0
spring.cloud.stream.bindings.messageProcessor-in-0.consumer.instanceIndex: 0

---

spring.config.activate.on-profile: streaming_instance_1
spring.cloud.stream.bindings.messageProcessor-in-0.consumer.instanceIndex: 1


---
spring.config.activate.on-profile: kafka
management.health.rabbit.enabled: false
spring.cloud.stream.defaultBinder: kafka
spring.kafka.bootstrap-servers: kafka:9092
spring.cloud.stream.kafka.binder.replication-factor: 1

