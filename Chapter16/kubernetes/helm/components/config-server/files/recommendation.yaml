server.port: 7002
server.error.include-message: always

# It is used to give each microservice a virtual hostname, a name used by the Eureka service to identify each
# microservice. Eureka clients will use this virtual hostname in the URLs that are used to make HTTP
# calls to the microservice.The application name property has been moved to the application.yml inside the
## microservices/recommendation.yaml
#spring.application.name: recommendation


# With the database in place, we now need to set up the configuration for the core microservices so they
# know how to connect to their databases. This is set up in each core microservice’s configuration file,
# application.yml
# Setting the log level for MongoTemplate to DEBUG will allow us to see which MongoDB statements are executed in the log.
spring.data.mongodb:
  host: localhost
  port: 27017
  database: recommendation-db

logging:
  level:
    root: INFO
    com.microservices: DEBUG
    org.springframework.data.mongodb.core.MongoTemplate: DEBUG

# this specifies which spring function bean we want to run, message processor bean that I have defined. That means
# whatever message come from rabbitMq or Kafka, spring cloud stream will invoke this function.
spring.cloud.function.definition: messageProcessor

# Means my message processor has to consume message from the recommendation topic, for this I have defined
# a binding. Mere messageProcessor jo ke -in- input type ka hai usko destination:recommendations queue/topic ke
# sath bind kardo. Ye hum springCloud stream ko bata rahy hain.
spring.cloud.stream:
  defaultBinder: rabbit #binder means messaging system, by default we will use rabbitMQ
  default.contentType: application/json
  bindings.messageProcessor-in-0: # Binding = connection between your app function and the message destination (topic/queue).
    destination: recommendations  # Name of the topic/queue where message will be consumed.
    group: recommendationsGroup

spring.cloud.stream.bindings.messageProcessor-in-0.consumer:
  maxAttempts: 3
  backOffInitialInterval: 500
  backOffMaxInterval: 1000
  backOffMultiplier: 2.0

spring.cloud.stream.rabbit.bindings.messageProcessor-in-0.consumer:
  autoBindDlq: true
  republishToDlq: true

spring.cloud.stream.kafka.bindings.messageProcessor-in-0.consumer:
  enableDlq: true

spring.cloud.stream.kafka.binder:
  brokers: 127.0.0.1
  defaultBrokerPort: 9092

spring.rabbitmq:
  host: 127.0.0.1
  port: 5672
  username: guest
  password: '{cipher}17fcf0ae5b8c5cf87de6875b699be4a1746dd493a99d926c7a26a68c422117ef'

management.endpoint.health.show-details: "ALWAYS"
management.endpoints.web.exposure.include: "*"
# A liveness probe tells Kubernetes if a Pod needs to be replaced, and a readiness probe tells Kubernetes
# if its Pod is ready to accept requests. To simplify this work, Spring Boot has added support to implement
# liveness and readiness probes. The probes are exposed on the URLs /actuator/health/liveness and
# /actuator/health/readiness respectively. They can either be declared by configuration or implementation
# in source code, if increased control is required compared to what configuration gives.
# When declaring the probes by configuration, a health group can be declared for each probe,
# specifying what existing health indicators it should include. For example, a readiness probe should
# report DOWN if a microservice can’t access its MongoDB database. In this case, the health group for the
# readiness probe should include the mongo health indicator.
management.endpoint.health.probes.enabled: true
management.endpoint.health.group.readiness.include: readinessState, rabbit, db, mongo

# Configuration for using Micrometer Tracing and Zipkin is added to the configuration file, config-repo/*.yml.
# In the default profile, it is specified that trace information will be sent to Zipkin using the following URL.
management.zipkin.tracing.endpoint: http://zipkin-server:9411/api/v2/spans

# By default, Micrometer Tracing only sends 10% of the traces to Zipkin. To ensure that all traces are
# sent to Zipkin, the following property is added to the default profile
management.tracing.sampling.probability: 1.0

# We also want trace and span IDs to be written to logs; this will enable us to correlate log output from
# cooperating microservices that, for example, fulfill a request sent to the external API.
# We can include the trace and span IDs in the log output by specifying the following log format:
# With the above log format, the log output will look like:
# 2023-04-22T14:02:07.417Z  INFO [product-composite,01234,56789]
# Where product-composite is the name of the microservice, 01234 is the trace ID, and 56789 is the span ID.
logging.pattern.level: "%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]"

server.shutdown: graceful
spring.lifecycle.timeout-per-shutdown-phase: 10s

# In the docker-profile which is after --- we are overriding only few properties, the properties that are
# not overridden (defined again) will take the values from above (before ---). Springboot merges both these
# documents and apply profile based overrides.
# Values in a profile override values from the default profile. But other values comes as it is.
# By using YAML files, multiple Spring profiles can be placed in the same file, separated by ---

---

# To handle the different configurations that are required when running locally without Docker and
# when running the microservices as Docker containers, we will use Spring profiles. Now, we will create a new
# Spring profile named docker to be used when we run our microservices as containers in Docker.
# yaha per baki sari configuration default profile wali ajae gi except jo neechay define ho rhaa ha wo bas override
# hoga default profile ke upar.

spring.config.activate.on-profile: docker

server.port: 8080

# When running inside Docker using the Spring profile, docker, the database is expected to be reachable on mongodb:27017.
spring.data.mongodb.host: mongodb

# In the default Spring profile, we specify hostnames to be used when we run our system landscape
# without Docker on localhost with the IP address 127.0.0.1. In the docker Spring profile, we specify
# the hostnames we will use when running in Docker and using Docker Compose, that is, rabbitmq and kafka.
spring.rabbitmq.host: rabbitmq
spring.cloud.stream.kafka.binder.brokers: kafka


---
spring.config.activate.on-profile: streaming_partitioned

spring.cloud.stream.bindings.messageProcessor-in-0.consumer:
  partitioned: true
  instanceCount: 2

---
spring.config.activate.on-profile: streaming_instance_0

spring.cloud.stream.bindings.messageProcessor-in-0.consumer.instanceIndex: 0

---
spring.config.activate.on-profile: streaming_instance_1

spring.cloud.stream.bindings.messageProcessor-in-0.consumer.instanceIndex: 1

---
spring.config.activate.on-profile: kafka

management.health.rabbit.enabled: false
spring.cloud.stream.defaultBinder: kafka
spring.kafka.bootstrap-servers: kafka:9092
spring.cloud.stream.kafka.binder.replication-factor: 1