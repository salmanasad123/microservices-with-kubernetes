server.port: 7000
server.error.include-message: always

spring.autoconfigure.exclude:
  - org.springframework.boot.autoconfigure.security.servlet.OAuth2ResourceServerJwtConfiguration

# It is used to give each microservice a virtual hostname, a name used by the Eureka service to identify each
# microservice. Eureka clients will use this virtual hostname in the URLs that are used to make HTTP
# calls to the microservice. The application name property has been moved to the application.yml inside the
# microservices/product-composite.yml
# spring.application.name: product-composite

# For Eureka clients, the credentials can be specified in the connection URL for the Eureka server. This
# is specified in each client’s configuration file, application.ym
app:
  eureka-username: u
  eureka-password: '{cipher}bf298f6d5f878b342f9e44bec08cb9ac00b4ce57e98316f030194a225fac89fb'
  eureka-server: localhost
  auth-server: localhost

eureka:
  client:
    serviceUrl:
      defaultZone: "http://${app.eureka-username}:${app.eureka-password}@${app.eureka-server}:8761/eureka/"
    initialInstanceInfoReplicationIntervalSeconds: 5
    registryFetchIntervalSeconds: 5
  instance:
    leaseRenewalIntervalInSeconds: 5
    leaseExpirationDurationInSeconds: 5

# To avoid hardcoding the address information for the core services into the source code of the composite
# microservice, the composite service uses a property file where information on how to find the core services is stored.

# We can now get rid of our hardcoded configuration of available microservices in application.
# yml. It looks like this because we are using eureka to find the instances of microservices.
#app:
#  product-service:
#    host: localhost
#    port: 7001
#  recommendation-service:
#    host: localhost
#    port: 7002
#  review-service:
#    host: localhost
#    port: 7003

# We also need to set up the configuration for the messaging system, to be able to publish events; this
# is similar to what we did for the consumers. Declaring RabbitMQ as the default messaging system,
# JSON as the default content type, and Kafka and RabbitMQ for connectivity information is the same
# as for the consumers.
# Spring Cloud Stream ek abstraction deta hai taake tu code mein kafka/rabbitmq specific code na likhe.
# Tu sirf “message channel” define karta hai (like products-out-0),
# aur config ke zariye decide karta hai ke wo channel RabbitMQ ya Kafka ke kis topic/queue se bind hoga.
spring.cloud.stream:
  defaultBinder: rabbit
  default.contentType: application/json
  bindings:
    products-out-0:
      destination: products
      producer:
        required-groups: auditGroup
    recommendations-out-0:
      destination: recommendations
      producer:
        required-groups: auditGroup
    reviews-out-0:
      destination: reviews
      producer:
        required-groups: auditGroup

spring.cloud.stream.kafka.binder:
  brokers: 127.0.0.1
  defaultBrokerPort: 9092

spring.rabbitmq:
  host: 127.0.0.1
  port: 5672
  username: guest
  password: guest

# timeoutDuration: Specifies how long a TimeLimiter instance waits for a call to complete before it throws
# a timeout exception. We will set it to 2s.

resilience4j.timelimiter:
  instances:
    product:
      timeoutDuration: 2s

# maxAttempts: The number of attempts before giving up, including the first call. We will set this parameter to 3,
# allowing a maximum of two retry attempts after an initial failed call.
# • waitDuration: The wait time before the next retry attempt. We will set this value to 1000 ms, meaning that
# we will wait 1 second between retries.
# • retryExceptions: A list of exceptions that will trigger a retry. We will only trigger retries on
#InternalServerError exceptions, that is, when HTTP requests respond with a 500 status code
resilience4j.retry:
  instances:
    product:
      maxAttempts: 3
      waitDuration: 1000
      retryExceptions:
        - org.springframework.web.reactive.function.client.WebClientResponseException$InternalServerError

management.health.circuitbreakers.enabled: true

resilience4j.circuitbreaker:
  instances:
    product:
      allowHealthIndicatorToFail: false
      registerHealthIndicator: true
      slidingWindowType: COUNT_BASED
      slidingWindowSize: 5
      failureRateThreshold: 50
      waitDurationInOpenState: 10000
      permittedNumberOfCallsInHalfOpenState: 3
      automaticTransitionFromOpenToHalfOpenEnabled: true
      ignoreExceptions:
        - com.example.api.api.exceptions.InvalidInputException
        - com.example.api.api.exception.NotFoundException

logging:
  level:
    root: INFO
    com.example: DEBUG

# spring.security.oauth2.resourceserver.jwt.issuer-uri ka kaam ye hota hai ke resource server
# (yani yahan product-composite) ko batana “kis authorization server ne token issue kiya tha” —
# aur uska public key (JWK set) kahan se milega taake woh JWT verify kar sake.
spring.security.oauth2.resourceserver.jwt.issuer-uri: http://${app.auth-server}:9999

management.endpoint.health.show-details: "ALWAYS"
management.endpoints.web.exposure.include: "*"

# Configuration for using Micrometer Tracing and Zipkin is added to the configuration file, config-repo/*.yml.
# In the default profile, it is specified that trace information will be sent to Zipkin using the following URL.
management.zipkin.tracing.endpoint: http://zipkin-server:9411/api/v2/spans

# By default, Micrometer Tracing only sends 10% of the traces to Zipkin. To ensure that all traces are
# sent to Zipkin, the following property is added to the default profile
management.tracing.sampling.probability: 1.0

# We also want trace and span IDs to be written to logs; this will enable us to correlate log output from
# cooperating microservices that, for example, fulfill a request sent to the external API.
# We can include the trace and span IDs in the log output by specifying the following log format:
# With the above log format, the log output will look like:
# 2023-04-22T14:02:07.417Z  INFO [product-composite,01234,56789]
# Where product-composite is the name of the microservice, 01234 is the trace ID, and 56789 is the span ID.
logging.pattern.level: "%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]"

#  For the product-composite-service, things are a bit more complicated since it needs to know where
# to find the core services. When we ran all the services on localhost, it was configured to use localhost
# and individual port numbers, 7001-7003, for each core service. When running in Docker, each service
# will have its own hostname but will be accessible on the same port number, 8080. Here, the docker
# profile for product-composite-service looks as follows

# Where did the hostnames product, recommendation, and review come from?
# These are specified in the docker-compose.yml file

---
spring.config.activate.on-profile: docker

server.port: 8080

app:
  eureka-server: eureka
  auth-server: auth-server

# The below hardcoded hosts are no longer needed as we are using eureka to get details.
#app:
#  product-service:
#    host: product
#    port: 8080
#  recommendation-service:
#    host: recommendation
#    port: 8080
#  review-service:
#    host: review
#    port: 8080


spring.rabbitmq.host: rabbitmq

spring.cloud.stream.kafka.binder.brokers: kafka

---
spring.config.activate.on-profile: streaming_partitioned

spring.cloud.stream.bindings.products-out-0.producer:
  partition-key-expression: headers['partitionKey']
  partition-count: 2

spring.cloud.stream.bindings.recommendations-out-0.producer:
  partition-key-expression: headers['partitionKey']
  partition-count: 2

spring.cloud.stream.bindings.reviews-out-0.producer:
  partition-key-expression: headers['partitionKey']
  partition-count: 2

---
spring.config.activate.on-profile: kafka

management.health.rabbit.enabled: false
spring.cloud.stream.defaultBinder: kafka
spring.kafka.bootstrap-servers: kafka:9092
spring.cloud.stream.kafka.binder.replication-factor: 1
